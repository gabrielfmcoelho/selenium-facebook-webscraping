{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping - grupos facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from scrapy.selector import Selector\n",
    "from time import sleep\n",
    "\n",
    "class GroupsSearch():\n",
    "    name = \"groups-search\"\n",
    "\n",
    "    def __init__(self, is_headless=True, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if is_headless:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        else:\n",
    "            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "        with open(\"config.json\") as f:\n",
    "            self.config = json.load(f)\n",
    "\n",
    "        self.groups = []\n",
    "\n",
    "    def login_facebook(self):\n",
    "        \"\"\"Log in to Facebook and store cookies for subsequent requests\"\"\"\n",
    "        self.driver.get(\"https://www.facebook.com/\")\n",
    "        sleep(1)  # Give enough time for the page to load\n",
    "\n",
    "        # Fill in email and password fields, then log in\n",
    "        email_input = self.driver.find_element(By.NAME, \"email\")\n",
    "        password_input = self.driver.find_element(By.NAME, \"pass\")\n",
    "\n",
    "        email_input.send_keys(self.config.get(\"credentials\").get(\"email\"))\n",
    "        sleep(1)  # Slow down the typing\n",
    "        password_input.send_keys(self.config.get(\"credentials\").get(\"password\"))\n",
    "\n",
    "        # Click the login button\n",
    "        login_button = self.driver.find_element(By.NAME, \"login\")\n",
    "        sleep(1)\n",
    "        login_button.click()\n",
    "\n",
    "        sleep(1)  # Wait for the login to complete\n",
    "\n",
    "        # Store the cookies after login\n",
    "        self.cookies = self.driver.get_cookies()\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Initial entry point for Scrapy\"\"\"\n",
    "        max_attempts = 3\n",
    "        \n",
    "        # First, log in to Facebook\n",
    "        self.login_facebook()\n",
    "\n",
    "        # Once logged in, start processing the tag names from the config file\n",
    "        for site in self.config[\"tags\"]:\n",
    "            attempts = 0\n",
    "            success = False\n",
    "            tagname = site[\"tagname\"]\n",
    "            articles_limit = site[\"articles_limit\"]\n",
    "            \n",
    "            # Perform a search for each tagname in the input field\n",
    "            self.search_tag(tagname)\n",
    "            \n",
    "            # After search, look for \"Grupos\" and click on \"Ver tudo\"\n",
    "            while attempts < max_attempts: # quebra as vezes, tenta de novo\n",
    "                try:\n",
    "                    self.click_see_all_groups()\n",
    "                    success = True  # Se o clique for bem-sucedido, marcar como sucesso\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    print(f'Falha em encontrar o botão \"ver tudo\" (grupos) para a tag {tagname}. Tentativa {attempts}')\n",
    "\n",
    "            if success:\n",
    "                # Proceed to scrape articles from the groups page\n",
    "                self.parse_groups_page(articles_limit)\n",
    "                self.save_groups(tagname)\n",
    "                self.save_groups(tagname, txt=True)\n",
    "            self.groups = []\n",
    "\n",
    "            # Go back to root before processing the next tagname\n",
    "            self.go_back_to_root()\n",
    "        \n",
    "    def go_to_groups_page(self, tagname):\n",
    "        if not tagname:\n",
    "            tagname = self.config[\"tags\"][0][\"tagname\"]\n",
    "        \n",
    "        self.search_tag(tagname)\n",
    "        self.click_see_all_groups()\n",
    "\n",
    "    def search_tag(self, tagname):\n",
    "        \"\"\"Search for a tagname in the combobox input field\"\"\"\n",
    "        # Find the input field with role=\"combobox\"\n",
    "        search_input = self.driver.find_element(By.XPATH, '//input[@role=\"combobox\"]')\n",
    "        \n",
    "        # Clear the input field and insert the tagname\n",
    "        search_input.clear()\n",
    "        sleep(1)  # Slow down to avoid detection\n",
    "        search_input.send_keys(tagname)\n",
    "        sleep(1)  # Give time for suggestions or auto-complete\n",
    "\n",
    "        # Simulate pressing Enter\n",
    "        search_input.send_keys(u'\\ue007')\n",
    "        sleep(2)  # Wait for the search results to load\n",
    "\n",
    "    def click_see_all_groups(self):\n",
    "        \"\"\"Click the 'Ver tudo' link inside the article with 'Grupos' in the feed\"\"\"\n",
    "        # Locate the feed div\n",
    "        feed_div = self.driver.find_element(By.XPATH, '//div[@role=\"feed\"]')\n",
    "        sleep(1)\n",
    "\n",
    "        # Locate the div with role=\"article\" that contains the span with text \"Grupos\"\n",
    "        article_div = feed_div.find_element(By.XPATH, './/div[@role=\"article\" and .//span[text()=\"Grupos\"]]')\n",
    "        sleep(1)\n",
    "\n",
    "        # Inside the article div, find the first 'a' with role=\"link\" and aria-label=\"Ver tudo\"\n",
    "        see_all_link = article_div.find_element(By.XPATH, './/a[@role=\"link\" and @aria-label=\"Ver tudo\"]')\n",
    "        sleep(1)\n",
    "\n",
    "        # Click the \"Ver tudo\" link\n",
    "        see_all_link.click()\n",
    "        sleep(2)  # Wait for the new page to load\n",
    "\n",
    "    def go_back_to_root(self):\n",
    "        \"\"\"Click the link to return to the root page\"\"\"\n",
    "        root_link = self.driver.find_element(By.XPATH, '//a[@role=\"link\" and @href=\"/\"]')\n",
    "        sleep(1)  # Slow down before clicking\n",
    "        root_link.click()\n",
    "        sleep(2)  # Wait for the root page to load\n",
    "\n",
    "    def parse_groups_page(self, articles_limit=10):\n",
    "        \"\"\"Parse the response for articles on the groups page\"\"\"\n",
    "        sel = Selector(text=self.driver.page_source)\n",
    "        self.articles = sel.xpath('//div[@role=\"feed\"]//div[@role=\"article\"]')\n",
    "        count = 0\n",
    "\n",
    "        print(f\"Found {len(self.articles)} articles on the page, looking for {articles_limit} articles\")\n",
    "        # for each batch of 20 articles, scroll down to load more articles\n",
    "        scroll_limit = articles_limit // 2\n",
    "        while len(self.articles) < articles_limit:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            sleep(1)\n",
    "            sel = Selector(text=self.driver.page_source)\n",
    "            self.articles = sel.xpath('//div[@role=\"feed\"]//div[@role=\"article\"]')\n",
    "            print(f\"Found {len(self.articles)} articles on the page, looking for {articles_limit} articles\")\n",
    "            scroll_limit -= 1\n",
    "            if scroll_limit == 0:\n",
    "                print(\"Reached the end of the page, stopping the search\")\n",
    "                break\n",
    "\n",
    "        for article in self.articles:\n",
    "            if count >= articles_limit:\n",
    "                break\n",
    "            self.groups.append(self.unlock_group(count))\n",
    "            count += 1\n",
    "    \n",
    "    def unlock_group(self, count):\n",
    "        article = self.articles[count]\n",
    "        groupname = article.xpath('.//a[@role=\"presentation\"]//text()').get()\n",
    "        print(f\"Group name: {groupname}\")\n",
    "        grouplink = article.xpath('.//a[@role=\"presentation\"]/@href').get()\n",
    "        print(f\"Group link: {grouplink}\")\n",
    "        \n",
    "        # Store or yield the data   \n",
    "        return {\n",
    "            'groupname': groupname,\n",
    "            'grouplink': grouplink\n",
    "        }\n",
    "    \n",
    "    def save_groups(self, tagname, txt=False):\n",
    "        folder = \"./data/\"\n",
    "        firstime = True\n",
    "        if txt:\n",
    "            with open(f'{folder}txt/grupos_{tagname}.txt', 'w') as f:\n",
    "                for group in self.groups:\n",
    "                    if firstime:\n",
    "                        f.write(f\"{group['grouplink']}\")\n",
    "                    else:\n",
    "                        f.write(f\"\\n{group['grouplink']}\")\n",
    "        else:\n",
    "            with open(f'{folder}json/grupos_{tagname}.json', 'w') as f:\n",
    "                json.dump(self.groups, f)\n",
    "\n",
    "    def closed(self, reason):\n",
    "        \"\"\"Cleanup the driver when the spider is closed\"\"\"\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_HEADLESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falha em encontrar o botão \"ver tudo\" (grupos) para a tag floriano. Tentativa 1\n",
      "Falha em encontrar o botão \"ver tudo\" (grupos) para a tag floriano. Tentativa 2\n",
      "Falha em encontrar o botão \"ver tudo\" (grupos) para a tag floriano. Tentativa 3\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=128.0.6613.119)\nStacktrace:\n#0 0x60e7e92332da <unknown>\n#1 0x60e7e8f01200 <unknown>\n#2 0x60e7e8ed6a1f <unknown>\n#3 0x60e7e8f7deed <unknown>\n#4 0x60e7e8f936a9 <unknown>\n#5 0x60e7e8f75673 <unknown>\n#6 0x60e7e8f43473 <unknown>\n#7 0x60e7e8f4447e <unknown>\n#8 0x60e7e91fa0db <unknown>\n#9 0x60e7e91fe071 <unknown>\n#10 0x60e7e91e69d5 <unknown>\n#11 0x60e7e91febf2 <unknown>\n#12 0x60e7e91cbb6f <unknown>\n#13 0x60e7e9222248 <unknown>\n#14 0x60e7e9222417 <unknown>\n#15 0x60e7e92320cc <unknown>\n#16 0x78eeed894ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gs \u001b[38;5;241m=\u001b[39m GroupsSearch(IS_HEADLESS)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m, in \u001b[0;36mGroupsSearch.start_requests\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m articles_limit \u001b[38;5;241m=\u001b[39m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticles_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Perform a search for each tagname in the input field\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# After search, look for \"Grupos\" and click on \"Ver tudo\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m attempts \u001b[38;5;241m<\u001b[39m max_attempts: \u001b[38;5;66;03m# quebra as vezes, tenta de novo\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m, in \u001b[0;36mGroupsSearch.search_tag\u001b[0;34m(self, tagname)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search for a tagname in the combobox input field\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Find the input field with role=\"combobox\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m search_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//input[@role=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombobox\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Clear the input field and insert the tagname\u001b[39;00m\n\u001b[1;32m    103\u001b[0m search_input\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/envs/scrap-facebook/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:748\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    745\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    746\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/scrap-facebook/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/scrap-facebook/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=128.0.6613.119)\nStacktrace:\n#0 0x60e7e92332da <unknown>\n#1 0x60e7e8f01200 <unknown>\n#2 0x60e7e8ed6a1f <unknown>\n#3 0x60e7e8f7deed <unknown>\n#4 0x60e7e8f936a9 <unknown>\n#5 0x60e7e8f75673 <unknown>\n#6 0x60e7e8f43473 <unknown>\n#7 0x60e7e8f4447e <unknown>\n#8 0x60e7e91fa0db <unknown>\n#9 0x60e7e91fe071 <unknown>\n#10 0x60e7e91e69d5 <unknown>\n#11 0x60e7e91febf2 <unknown>\n#12 0x60e7e91cbb6f <unknown>\n#13 0x60e7e9222248 <unknown>\n#14 0x60e7e9222417 <unknown>\n#15 0x60e7e92320cc <unknown>\n#16 0x78eeed894ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "gs = GroupsSearch(IS_HEADLESS)\n",
    "gs.start_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping - posts grupos facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disponibilização google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def combine_txt_files(local_folder, combined_file_path):\n",
    "    with open(combined_file_path, 'w') as combined_file:\n",
    "        # Iterar sobre arquivos na pasta local\n",
    "        unique_links = []\n",
    "        first = True\n",
    "        for filename in os.listdir(local_folder):\n",
    "            if filename.endswith('.txt'):  # Filtrar apenas arquivos .txt\n",
    "                file_path = os.path.join(local_folder, filename)\n",
    "                firstime = True if not first else False\n",
    "                first = False\n",
    "                # Ler o conteúdo de cada arquivo .txt\n",
    "                with open(file_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if line not in unique_links:\n",
    "                            unique_links.append(line)\n",
    "                            if firstime:\n",
    "                                firstime = False\n",
    "                                combined_file.write(\"\\n\" + line)\n",
    "                            combined_file.write(line)\n",
    "                    #combined_file.write(f.read() + '\\n')  # Adicionar quebra de linha entre os arquivos\n",
    "                print(f'Adicionado {filename} ao arquivo combinado.')\n",
    "\n",
    "def upload_file_to_gcs(file_path, bucket_name, destination_blob_name):\n",
    "    # Instanciar cliente Google Cloud Storage\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'personal-portfolio-396715-c502ea581c84.json'\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Criar blob no bucket para o arquivo\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Fazer upload do arquivo combinado\n",
    "    blob.upload_from_filename(file_path)\n",
    "    print(f'Uploaded {destination_blob_name} to {bucket_name}')\n",
    "\n",
    "# Função principal que combina arquivos e faz upload\n",
    "def combine_and_upload_txt_files(local_folder, bucket_name, destination_blob_name):\n",
    "    combined_file_path = os.path.join(f'{local_folder}/temp', 'compilado.txt')\n",
    "\n",
    "    # Combinar todos os arquivos .txt\n",
    "    combine_txt_files(local_folder, combined_file_path)\n",
    "\n",
    "    # Fazer upload do arquivo combinado para o Google Cloud Storage\n",
    "    upload_file_to_gcs(combined_file_path, bucket_name, destination_blob_name)\n",
    "\n",
    "    # Opcionalmente, remover o arquivo combinado localmente\n",
    "    os.remove(combined_file_path)\n",
    "    print(f'Arquivo combinado removido localmente: {combined_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adicionado grupos_the.txt ao arquivo combinado.\n",
      "Adicionado grupos_Poti Velho.txt ao arquivo combinado.\n",
      "Adicionado grupos_Angelim.txt ao arquivo combinado.\n",
      "Adicionado grupos_Campo Maior.txt ao arquivo combinado.\n",
      "Adicionado grupos_picos.txt ao arquivo combinado.\n",
      "Adicionado grupos_luis correia.txt ao arquivo combinado.\n",
      "Adicionado grupos_phb.txt ao arquivo combinado.\n",
      "Adicionado grupos_piaui.txt ao arquivo combinado.\n",
      "Adicionado grupos_Lourival Parente.txt ao arquivo combinado.\n",
      "Adicionado grupos_Bom Jesus.txt ao arquivo combinado.\n",
      "Adicionado grupos_teresina.txt ao arquivo combinado.\n",
      "Adicionado grupos_santa maria da codipi.txt ao arquivo combinado.\n",
      "Adicionado grupos_Piripiri.txt ao arquivo combinado.\n",
      "Adicionado grupos_Alto Alegre.txt ao arquivo combinado.\n",
      "Adicionado grupos_Vale do Gavião.txt ao arquivo combinado.\n",
      "Adicionado grupos_Gurupi.txt ao arquivo combinado.\n",
      "Adicionado grupos_parnaiba.txt ao arquivo combinado.\n",
      "Adicionado grupos_Promorar.txt ao arquivo combinado.\n",
      "Adicionado grupos_Macaúba.txt ao arquivo combinado.\n",
      "Adicionado grupos_oeiras.txt ao arquivo combinado.\n",
      "Adicionado grupos_Jacinta Andrade.txt ao arquivo combinado.\n",
      "Uploaded compilado_grupos_facebook.txt to disponibilizacao_dados\n",
      "Arquivo combinado removido localmente: data/txt/temp/compilado.txt\n"
     ]
    }
   ],
   "source": [
    "# Definir a pasta local e o nome do bucket\n",
    "local_folder = \"data/txt\"\n",
    "bucket_name = \"disponibilizacao_dados\"\n",
    "destination_blob_name = \"compilado_grupos_facebook.txt\"\n",
    "\n",
    "# Chamar a função para fazer o upload\n",
    "combine_and_upload_txt_files(local_folder, bucket_name, destination_blob_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap-facebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
